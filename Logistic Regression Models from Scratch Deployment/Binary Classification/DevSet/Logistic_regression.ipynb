{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c391a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e8e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    z = np.dot(X, w) + b\n",
    "    probs = 1 / (1 + np.exp(-z))\n",
    "    return (probs >= 0.5).astype(int)\n",
    "\n",
    "def predict_prop( w, b, X):\n",
    "    z = np.dot(X, w) + b\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "def compute_cost(w, b, X, y):\n",
    "    m = X.shape[0]\n",
    "    f_wb = predict_prop(w, b, X)\n",
    "    cost = -1/m * np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb))\n",
    "    return cost\n",
    "\n",
    "def compute_gradient(w, b, X, y):\n",
    "    m = X.shape[0]\n",
    "    f_wb = predict_prop(w, b, X)\n",
    "    \n",
    "    dj_dw = 1/m * np.dot(X.T, (f_wb - y))\n",
    "    dj_db = 1/m * np.sum(f_wb - y)\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(lr, x, y, w, b, previous_cost, threshold, max_iters=1000000):\n",
    "    m = x.shape[0]\n",
    "    iters = 0\n",
    "    cost_log = []\n",
    "    iteration_log = []\n",
    "    weight_log = []\n",
    "    bias_log = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "        w -= (lr/m) * dj_dw\n",
    "        b -= (lr/m) * dj_db\n",
    "\n",
    "        current_cost = compute_cost(x, y, w, b)\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print(f\"Iteration {iters} | Cost: {current_cost:.5f} | w: {w.ravel()} | b: {b:.5f}\")\n",
    "        #     if iters < 1001 and iters % 200 == 0 and iters > 1:\n",
    "        #         cost_log.append(current_cost)\n",
    "        #         iteration_log.append(iters)\n",
    "        #         weight_log.append(w.flatten().tolist())  # Save as list\n",
    "        #         bias_log.append(b)\n",
    "        #     elif iters < 10000 and iters > 1001 and iters % 1000 == 0:\n",
    "        #         cost_log.append(current_cost)\n",
    "        #         iteration_log.append(iters)\n",
    "        #         weight_log.append(w.flatten().tolist())  # Save as list\n",
    "        #         bias_log.append(b)\n",
    "        \n",
    "        # if iters % 10000 == 0:\n",
    "        #     cost_log.append(current_cost)\n",
    "        #     iteration_log.append(iters)\n",
    "        #     weight_log.append(w.flatten().tolist())  # Save as list\n",
    "        #     bias_log.append(b)\n",
    "            \n",
    "        if abs(current_cost-previous_cost) < threshold:\n",
    "            print(f\"Converged in {iters} iterations.\")\n",
    "            break\n",
    "\n",
    "        previous_cost = current_cost\n",
    "        iters += 1\n",
    "        if iters >= max_iters:\n",
    "            print(\"Stopped: Max iterations reached.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    return w , b #,cost_log , iteration_log , weight_log , bias_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29052c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"cleaned_titanic_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('Survived', axis=1).values\n",
    "y = df['Survived'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Step 1: Shuffle the data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Step 2: 80/20 split\n",
    "split_index = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Step 3: Z-score normalization (standardization) for X\n",
    "#mean = np.mean(X_train, axis=0)\n",
    "#std = np.std(X_train, axis=0)\n",
    "\n",
    "#X_train_scaled = (X_train - mean) / std\n",
    "#X_test_scaled = (X_test - mean) / std  # use train mean & std!\n",
    "\n",
    "# Step 4: Initialize parameters\n",
    "m, n = X_train.shape\n",
    "w = np.random.uniform(low=-1.0, high=1.0, size=(n, 1))\n",
    "b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1cd143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahad\\AppData\\Local\\Temp\\ipykernel_14944\\2791671511.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -1/m * np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb))\n",
      "C:\\Users\\fahad\\AppData\\Local\\Temp\\ipykernel_14944\\2791671511.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -1/m * np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,1) and (705,10) not aligned: 1 (dim 1) != 705 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wf , bf \u001b[38;5;241m=\u001b[39m gradient_descent(\u001b[38;5;241m0.02\u001b[39m, X_train, y_train, w, b, previous_cost\u001b[38;5;241m=\u001b[39mcompute_cost(w, b, X_train, y_train), threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(lr, x, y, w, b, previous_cost, threshold, max_iters)\u001b[0m\n\u001b[0;32m     32\u001b[0m bias_log \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     dj_dw, dj_db \u001b[38;5;241m=\u001b[39m compute_gradient(x, y, w, b)\n\u001b[0;32m     37\u001b[0m     w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (lr\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m dj_dw\n\u001b[0;32m     38\u001b[0m     b \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (lr\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m dj_db\n",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[1;34m(w, b, X, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradient\u001b[39m(w, b, X, y):\n\u001b[0;32m     18\u001b[0m     m \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m     f_wb \u001b[38;5;241m=\u001b[39m predict_prop(w, b, X)\n\u001b[0;32m     21\u001b[0m     dj_dw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, (f_wb \u001b[38;5;241m-\u001b[39m y))\n\u001b[0;32m     22\u001b[0m     dj_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(f_wb \u001b[38;5;241m-\u001b[39m y)\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mpredict_prop\u001b[1;34m(w, b, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_prop\u001b[39m( w, b, X):\n\u001b[1;32m----> 7\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, w) \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m      8\u001b[0m     sigmoid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mz))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sigmoid\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,1) and (705,10) not aligned: 1 (dim 1) != 705 (dim 0)"
     ]
    }
   ],
   "source": [
    "wf , bf = gradient_descent(0.02, X_train, y_train, w, b, previous_cost=compute_cost(w, b, X_train, y_train), threshold=1e-5, max_iters=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
