{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca153e4a",
   "metadata": {},
   "source": [
    "# **📊 Logistic Regression from Scratch — Polynomial Feature Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c391a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1160f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_to_npy(w, b, std, mean, weight_file=\"weights_poly_trad.npy\"\n",
    "                        , bias_file=\"bias_poly_trad.npy\"\n",
    "                        , std_file=\"std_poly_trad.npy\"\n",
    "                        , mean_file=\"mean_poly_trad.npy\"):\n",
    "    np.save(weight_file, w)\n",
    "    np.save(bias_file, np.array([b]))\n",
    "    np.save(std_file, std)\n",
    "    np.save(mean_file, mean)\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    Generate polynomial features for input matrix X up to a given degree.\n",
    "    Parameters:\n",
    "        X: numpy array of shape (m, n)\n",
    "        degree: int, highest polynomial degree to generate\n",
    "    Returns:\n",
    "        X_poly: numpy array of shape (m, num_features)\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    features = [np.ones((m, 1))]  # Bias term: degree 0\n",
    "    \n",
    "    for deg in range(1, degree + 1):\n",
    "        for combo in combinations_with_replacement(range(n), deg):\n",
    "            # Multiply columns based on index combo\n",
    "            col = np.ones(m)\n",
    "            for idx in combo:\n",
    "                col *= X[:, idx]\n",
    "            features.append(col.reshape(-1, 1))\n",
    "    \n",
    "    return np.hstack(features)\n",
    "\n",
    "def predict(X, w, b):\n",
    "    probs = predict_prob(X, w, b)\n",
    "    return (probs >= 0.5).astype(int)\n",
    "\n",
    "def predict_prob(X, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    z = np.clip(z, -500, 500)  # Prevent exp overflow\n",
    "    return sigmoid(z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    f_wb = predict_prob(X, w, b)\n",
    "    f_wb = np.clip(f_wb, 1e-15, 1 - 1e-15)  # Prevent log(0)\n",
    "    cost = (-1/m) * np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb))\n",
    "    return cost\n",
    "\n",
    "def compute_gradient(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    f_wb = predict_prob(X, w, b)\n",
    "    \n",
    "    dj_dw = (1/m) * np.dot(X.T, (f_wb - y)) \n",
    "    dj_db = (1/m) * np.sum(f_wb - y)\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(lr, x, y, w, b, method, previous_cost, threshold, max_iters):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    iters = 0\n",
    "\n",
    "    #Logs for tracking\n",
    "    cost_log = []\n",
    "    iteration_log = []\n",
    "    weight_log = []\n",
    "    bias_log = []\n",
    "\n",
    "    method = method.lower()\n",
    "\n",
    "    if method == 'adam':\n",
    "        \n",
    "        v_w, v_b, s_w, s_b = np.zeros_like(w), 0.0, np.zeros_like(w), 0.0\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        while True:\n",
    "\n",
    "            dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "\n",
    "            v_w_corrected, v_b_corrected, s_w_corrected, s_b_corrected, v_w, v_b, s_w, s_b = compute_adam_update(dj_dw, dj_db, v_w, v_b, s_w, s_b, beta1, beta2, iters)\n",
    "            \n",
    "            w -= lr * v_w_corrected / (np.sqrt(s_w_corrected) + epsilon)\n",
    "            b -= lr * v_b_corrected / (np.sqrt(s_b_corrected) + epsilon)\n",
    "            \n",
    "            current_cost = compute_cost(x, y, w, b)\n",
    "\n",
    "            # if iters % 100 == 0:\n",
    "            #     print(f\"Iteration {iters} | Cost: {current_cost:.5f} | b: {b:.5f}\")\n",
    "            #     #Log the cost, iteration, weights, and bias\n",
    "            #     if iters < 1001 and iters % 100 == 0 and iters > 1:\n",
    "            #         cost_log.append(current_cost)\n",
    "            #         iteration_log.append(iters)\n",
    "            #         weight_log.append(w.flatten().tolist())  # Save as list\n",
    "            #         bias_log.append(b)\n",
    "            #     elif iters < 10000 and iters > 1001 and iters % 500 == 0:\n",
    "            #         cost_log.append(current_cost)\n",
    "            #         iteration_log.append(iters)\n",
    "            #         weight_log.append(w.flatten().tolist())  # Save as list\n",
    "            #         bias_log.append(b)\n",
    "\n",
    "            if abs(current_cost - previous_cost) < threshold:\n",
    "                print(f\"Converged in {iters} iterations.\")\n",
    "                break\n",
    "            \n",
    "            previous_cost = current_cost\n",
    "            iters += 1\n",
    "\n",
    "            if iters >= max_iters:\n",
    "                print(\"Stopped: Max iterations reached.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    elif method == 'vanilla' or method == 'gd':\n",
    "        while True:\n",
    "            \n",
    "            dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "\n",
    "            w = w - (lr/m)*(dj_dw)\n",
    "            b = b - (lr/m)*(dj_db)\n",
    "\n",
    "            current_cost = compute_cost(x, y, w, b)\n",
    "\n",
    "            # if iters % 100 == 0:\n",
    "            #     print(f\"Iteration {iters} | Cost: {current_cost:.5f} | w: {w.ravel()} | b: {b:.5f}\")\n",
    "                \n",
    "                # Log the cost, iteration, weights, and bias\n",
    "                # if iters < 1001 and iters % 100 == 0 and iters > 1:\n",
    "                #     cost_log.append(current_cost)\n",
    "                #     iteration_log.append(iters)\n",
    "                #     weight_log.append(w.flatten().tolist())  # Save as list\n",
    "                #     bias_log.append(b)\n",
    "                # elif iters < 10000 and iters > 1001 and iters % 1000 == 0:\n",
    "                #     cost_log.append(current_cost)\n",
    "                #     iteration_log.append(iters)\n",
    "                #     weight_log.append(w.flatten().tolist())  # Save as list\n",
    "                #     bias_log.append(b)\n",
    "             \n",
    "            if abs(current_cost-previous_cost) < threshold:\n",
    "                print(f\"Converged in {iters} iterations.\")\n",
    "                break\n",
    "\n",
    "            previous_cost = current_cost\n",
    "            iters += 1\n",
    "            if iters >= max_iters:\n",
    "                print(\"Stopped: Max iterations reached.\")\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimization method: {method}\")\n",
    "\n",
    "    return w , b #,cost_log , iteration_log , weight_log , bias_log\n",
    "\n",
    "def compute_adam_update(dj_dw, dj_db, v_w, v_b, s_w, s_b, beta1 , beta2  , iters):\n",
    "    # Update biased first moment estimates\n",
    "    v_w = beta1 * v_w + (1 - beta1) * dj_dw\n",
    "    v_b = beta1 * v_b + (1 - beta1) * dj_db\n",
    "\n",
    "    # Update biased second moment estimates\n",
    "    s_w = beta2 * s_w + (1 - beta2) * (dj_dw ** 2)\n",
    "    s_b = beta2 * s_b + (1 - beta2) * (dj_db ** 2)\n",
    "\n",
    "    # Compute bias-corrected first moment estimates\n",
    "    v_w_corrected = v_w / (1 - beta1 ** (iters + 1))\n",
    "    v_b_corrected = v_b / (1 - beta1 ** (iters + 1))\n",
    "\n",
    "    # Compute bias-corrected second moment estimates\n",
    "    s_w_corrected = s_w / (1 - beta2 ** (iters + 1))\n",
    "    s_b_corrected = s_b / (1 - beta2 ** (iters + 1))\n",
    "\n",
    "    return v_w_corrected, v_b_corrected, s_w_corrected, s_b_corrected, v_w, v_b, s_w, s_b\n",
    "\n",
    "def adam_gradient_descent(lr, x, y, w, b, previous_cost, threshold, max_iters):\n",
    "    m = x.shape[0]\n",
    "    iters = 0\n",
    "    cost_log = []\n",
    "    iteration_log = []\n",
    "    weight_log = []\n",
    "    bias_log = []\n",
    "\n",
    "    # Initialize Adam parameters\n",
    "    v_w = np.zeros_like(w)\n",
    "    v_b = 0.0\n",
    "    s_w = np.zeros_like(w)\n",
    "    s_b = 0.0\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    while True:\n",
    "        dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "\n",
    "        # Update biased first moment estimates\n",
    "        v_w = beta1 * v_w + (1 - beta1) * dj_dw\n",
    "        v_b = beta1 * v_b + (1 - beta1) * dj_db\n",
    "\n",
    "        # Update biased second moment estimates\n",
    "        s_w = beta2 * s_w + (1 - beta2) * (dj_dw ** 2)\n",
    "        s_b = beta2 * s_b + (1 - beta2) * (dj_db ** 2)\n",
    "\n",
    "        # Compute bias-corrected first moment estimates\n",
    "        v_w_corrected = v_w / (1 - beta1 ** (iters + 1))\n",
    "        v_b_corrected = v_b / (1 - beta1 ** (iters + 1))\n",
    "\n",
    "        # Compute bias-corrected second moment estimates\n",
    "        s_w_corrected = s_w / (1 - beta2 ** (iters + 1))\n",
    "        s_b_corrected = s_b / (1 - beta2 ** (iters + 1))\n",
    "\n",
    "        # Update parameters\n",
    "        w -= lr * v_w_corrected / (np.sqrt(s_w_corrected) + epsilon)\n",
    "        b -= lr * v_b_corrected / (np.sqrt(s_b_corrected) + epsilon)\n",
    "\n",
    "        current_cost = compute_cost(x, y, w, b)\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "            print(f\"Iteration {iters} | Cost: {current_cost:.5f} | w: {w.ravel()} | b: {b:.5f}\")\n",
    "\n",
    "        if abs(current_cost - previous_cost) < threshold:\n",
    "            print(f\"Converged in {iters} iterations.\")\n",
    "            break\n",
    "\n",
    "        previous_cost = current_cost\n",
    "\n",
    "    return w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67842f",
   "metadata": {},
   "source": [
    "### **🛠️ Goal**\n",
    "We implemented logistic regression from scratch with vectorized NumPy, trained using the Adam optimizer, and evaluated the impact of increasing polynomial degree(upto degree 7) on model performance. The model was trained on standardized features and evaluated on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 177 iterations.\n",
      "\n",
      " Validation Set Results for Degree 1\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8182\n",
      "Precision      : 0.8039\n",
      "Recall         : 0.6508\n",
      "F1 Score       : 0.7193\n",
      "Confusion Matrix:\n",
      " [[103  10]\n",
      " [ 22  41]]\n",
      "Converged in 4304 iterations.\n",
      "\n",
      " Validation Set Results for Degree 2\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8523\n",
      "Precision      : 0.8776\n",
      "Recall         : 0.6825\n",
      "F1 Score       : 0.7679\n",
      "Confusion Matrix:\n",
      " [[107   6]\n",
      " [ 20  43]]\n",
      "Converged in 5769 iterations.\n",
      "\n",
      " Validation Set Results for Degree 3\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.7898\n",
      "Precision      : 0.7955\n",
      "Recall         : 0.5556\n",
      "F1 Score       : 0.6542\n",
      "Confusion Matrix:\n",
      " [[104   9]\n",
      " [ 28  35]]\n",
      "Converged in 2483 iterations.\n",
      "\n",
      " Validation Set Results for Degree 4\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8125\n",
      "Precision      : 0.8409\n",
      "Recall         : 0.5873\n",
      "F1 Score       : 0.6916\n",
      "Confusion Matrix:\n",
      " [[106   7]\n",
      " [ 26  37]]\n",
      "Converged in 1084 iterations.\n",
      "\n",
      " Validation Set Results for Degree 5\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8295\n",
      "Precision      : 0.8837\n",
      "Recall         : 0.6032\n",
      "F1 Score       : 0.7170\n",
      "Confusion Matrix:\n",
      " [[108   5]\n",
      " [ 25  38]]\n",
      "Converged in 1161 iterations.\n",
      "\n",
      " Validation Set Results for Degree 6\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8295\n",
      "Precision      : 0.8511\n",
      "Recall         : 0.6349\n",
      "F1 Score       : 0.7273\n",
      "Confusion Matrix:\n",
      " [[106   7]\n",
      " [ 23  40]]\n",
      "Converged in 595 iterations.\n",
      "\n",
      " Validation Set Results for Degree 7\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8295\n",
      "Precision      : 0.8667\n",
      "Recall         : 0.6190\n",
      "F1 Score       : 0.7222\n",
      "Confusion Matrix:\n",
      " [[107   6]\n",
      " [ 24  39]]\n",
      "Converged in 959 iterations.\n",
      "\n",
      " Validation Set Results for Degree 8\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.8239\n",
      "Precision      : 0.8200\n",
      "Recall         : 0.6508\n",
      "F1 Score       : 0.7257\n",
      "Confusion Matrix:\n",
      " [[104   9]\n",
      " [ 22  41]]\n"
     ]
    }
   ],
   "source": [
    "degree = 7\n",
    "\n",
    "for degree in range(degree+1):\n",
    "    # Load data\n",
    "    df = pd.read_csv(\"cleaned_titanic_data.csv\")\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop('Survived', axis=1).values\n",
    "    y = df['Survived'].values.reshape(-1, 1)\n",
    "\n",
    "    X = polynomial_features(X, degree+1)\n",
    "\n",
    "    # Step 1: Shuffle the data\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "\n",
    "    # Step 2: 60% for training set, 40% for validation and test sets\n",
    "    split_index = int(0.6 * X.shape[0])\n",
    "    X_train, X_ = X[:split_index], X[split_index:]\n",
    "    y_train, y_ = y[:split_index], y[split_index:]\n",
    "\n",
    "    #20% for validation and 20% for test set\n",
    "    split_index = int(0.5 * X_.shape[0])\n",
    "    X_test, X_val = X_[split_index:], X_[:split_index]\n",
    "    y_test, y_val = y_[split_index:], y_[:split_index]\n",
    "\n",
    "    # Step 3: Z-score normalization (standardization) for X\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    std[std == 0] = 1.0\n",
    "\n",
    "    X_train_scaled = (X_train - mean) / std\n",
    "    X_test_scaled = (X_test - mean) / std  # use train mean & std!\n",
    "\n",
    "    X_val_scaled = (X_val - mean) / std  # use train mean & std!\n",
    "    X_test_scaled = (X_test - mean) / std  # use train mean & std!\n",
    "\n",
    "    # Step 4: Initialize parameters\n",
    "    m, n = X_train_scaled.shape\n",
    "    w = np.random.uniform(low=-1.0, high=1.0, size=(n, 1))\n",
    "    b = 0.0\n",
    "\n",
    "    wf, bf = gradient_descent(\n",
    "    0.01,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    w,\n",
    "    b,\n",
    "    method='adam',  # 'vanilla' or 'adam'\n",
    "    previous_cost=compute_cost(X_train, y_train, w, b),\n",
    "    threshold=1e-6,\n",
    "    max_iters=1000000\n",
    "    )\n",
    "\n",
    "    y_pred_val = predict(X_val_scaled, wf, bf)  # Use your predict function\n",
    "\n",
    "\n",
    "    # Compute classification metrics\n",
    "    acc = accuracy_score(y_val, y_pred_val)\n",
    "    prec = precision_score(y_val, y_pred_val)\n",
    "    rec = recall_score(y_val, y_pred_val)\n",
    "    f1 = f1_score(y_val, y_pred_val)\n",
    "    cm = confusion_matrix(y_val, y_pred_val)\n",
    "    print(\"\\n Validation Set Results for Degree\", degree + 1)\n",
    "    # Print results\n",
    "    print(\"\\n📊 Classification Metrics (Validation Set):\")\n",
    "    print(f\"Accuracy       : {acc:.4f}\")\n",
    "    print(f\"Precision      : {prec:.4f}\")\n",
    "    print(f\"Recall         : {rec:.4f}\")\n",
    "    print(f\"F1 Score       : {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b23ddc",
   "metadata": {},
   "source": [
    "### **🔍 Metric Definitions**\n",
    "Accuracy: Overall percentage of correct predictions.\n",
    "\n",
    "Precision: Proportion of predicted positives that were actually positive.\n",
    "\n",
    "Recall: Proportion of actual positives that were correctly predicted.\n",
    "\n",
    "F1 Score: Harmonic mean of precision and recall — balances the two.\n",
    "\n",
    "### **🧠 Key Observations**\n",
    "Degree 1 performs well with a simple linear model, achieving good metrics, but misses some nonlinear patterns.\n",
    "\n",
    "Degree 2 gives the best F1 score (0.7679) and highest accuracy (85.23%), striking an excellent balance between precision and recall.\n",
    "\n",
    "Degree 3 dips sharply in recall and F1 — likely due to overfitting or learning noisy interactions.\n",
    "\n",
    "Degrees 4–8 stabilize performance, but none outperform degree 2 in terms of F1 score or simplicity.\n",
    "\n",
    "Degree 5 achieves the highest precision (0.8837), meaning it makes fewer false positives, but recall is slightly worse than degree 2.\n",
    "\n",
    "### **🏆 Conclusion**\n",
    "Polynomial degree 2 is the optimal choice in this setup, balancing generalization, simplicity, and performance.\n",
    "\n",
    "It captures essential non-linearities without overfitting.\n",
    "\n",
    "Higher degrees offer marginal improvements but introduce complexity and overfitting risk.\n",
    "\n",
    "### **📌 Future Work**\n",
    "✅ Compare with scikit-learn’s LogisticRegression\n",
    "\n",
    "✅ Upload best-performing model to Hugging Face 🤗 spaces\n",
    "\n",
    "🔄 Extend with L2 regularization (Ridge):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dd3f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "48d137f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 4304 iterations.\n",
      "\n",
      " Validation Set Results for Degree 2\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.7345\n",
      "Precision      : 0.7843\n",
      "Recall         : 0.5263\n",
      "F1 Score       : 0.6299\n",
      "Confusion Matrix:\n",
      " [[90 11]\n",
      " [36 40]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_titanic_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('Survived', axis=1).values\n",
    "y = df['Survived'].values.reshape(-1, 1)\n",
    "\n",
    "X = polynomial_features(X, degree=2)\n",
    "\n",
    "# Step 1: Shuffle the data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "\n",
    "# Step 2: 60% for training set, 40% for validation and test sets\n",
    "split_index = int(0.6 * X.shape[0])\n",
    "X_train, X_ = X[:split_index], X[split_index:]\n",
    "y_train, y_ = y[:split_index], y[split_index:]\n",
    "\n",
    "#20% for validation and 20% for test set\n",
    "split_index = int(0.5 * X_.shape[0])\n",
    "X_test, X_val = X_[split_index:], X_[:split_index]\n",
    "y_test, y_val = y_[split_index:], y_[:split_index]\n",
    "\n",
    "# Step 3: Z-score normalization (standardization) for X\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "std[std == 0] = 1.0\n",
    "\n",
    "X_train_scaled = (X_train - mean) / std\n",
    "X_test_scaled = (X_test - mean) / std  # use train mean & std!\n",
    "X_val_scaled = (X_val - mean) / std  # use train mean & std!\n",
    "\n",
    "# Step 4: Initialize parameters\n",
    "m, n = X_train_scaled.shape\n",
    "w = np.random.uniform(low=-1.0, high=1.0, size=(n, 1))\n",
    "b = 0.0\n",
    "\n",
    "wf, bf = gradient_descent(\n",
    "0.01,\n",
    "X_train_scaled,\n",
    "y_train,\n",
    "w,\n",
    "b,\n",
    "method='adam',  # 'vanilla' or 'adam'\n",
    "previous_cost=compute_cost(X_train, y_train, w, b),\n",
    "threshold=1e-6,\n",
    "max_iters=1000000\n",
    ")\n",
    "\n",
    "y_pred_test = predict(X_test_scaled, wf, bf)  # Use your predict function\n",
    "\n",
    "\n",
    "# Compute classification metrics\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "rec = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\n Validation Set Results for Degree 2\")\n",
    "# Print results\n",
    "print(\"\\n📊 Classification Metrics (Validation Set):\")\n",
    "print(f\"Accuracy       : {acc:.4f}\")\n",
    "print(f\"Precision      : {prec:.4f}\")\n",
    "print(f\"Recall         : {rec:.4f}\")\n",
    "print(f\"F1 Score       : {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d58af",
   "metadata": {},
   "source": [
    "## Final Model Evaluation (Degree 2 Polynomial Features)\n",
    "\n",
    "After selecting the degree 2 polynomial model based on validation set performance,\n",
    "we retrained the model on the full 80% of data (train + validation) and evaluated on the held-out 20% test set.\n",
    "\n",
    "### ✅ Validation Set Metrics\n",
    "- Accuracy: 85.23%\n",
    "- Precision: 87.76%\n",
    "- Recall: 68.25%\n",
    "- F1 Score: 76.79%\n",
    "- Confusion Matrix: [[107, 6], [20, 43]]\n",
    "\n",
    "### 🧪 Test Set Metrics (Final Evaluation)\n",
    "- Accuracy: 73.45%\n",
    "- Precision: 78.43%\n",
    "- Recall: 52.63%\n",
    "- F1 Score: 62.99%\n",
    "- Confusion Matrix: [[90, 11], [36, 40]]\n",
    "\n",
    "### 🔍 Interpretation\n",
    "- The model experienced a performance drop when evaluated on the unseen test set.\n",
    "- This is expected and highlights the importance of final testing after model selection.\n",
    "- Despite the drop, the model generalizes reasonably well, with strong precision and acceptable recall.\n",
    "\n",
    "### 📌 Final Verdict\n",
    "Degree 2 was selected as the optimal model due to its balance of performance and simplicity.\n",
    "The results on the test set confirm its ability to generalize with room for further improvement (e.g. regularization, more data, or tuning thresholds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92aa64",
   "metadata": {},
   "source": [
    "# **Training Final Model Parameters for Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b22f6cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 4414 iterations.\n",
      "\n",
      " Validation Set Results for Degree 2\n",
      "\n",
      "📊 Classification Metrics (Validation Set):\n",
      "Accuracy       : 0.7288\n",
      "Precision      : 0.7258\n",
      "Recall         : 0.5921\n",
      "F1 Score       : 0.6522\n",
      "Confusion Matrix:\n",
      " [[84 17]\n",
      " [31 45]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_titanic_data.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('Survived', axis=1).values\n",
    "y = df['Survived'].values.reshape(-1, 1)\n",
    "\n",
    "X = polynomial_features(X, degree=2)\n",
    "\n",
    "# Step 1: Shuffle the data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "\n",
    "# Step 2: 80% for training set, 20% for validation and test sets\n",
    "split_index = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "\n",
    "# Step 3: Z-score normalization (standardization) for X\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "std[std == 0] = 1.0\n",
    "\n",
    "X_train_scaled = (X_train - mean) / std\n",
    "X_test_scaled = (X_test - mean) / std  # use train mean & std!\n",
    "\n",
    "# Step 4: Initialize parameters\n",
    "m, n = X_train_scaled.shape\n",
    "w = np.random.uniform(low=-1.0, high=1.0, size=(n, 1))\n",
    "b = 0.0\n",
    "\n",
    "wf, bf = gradient_descent(\n",
    "0.01,\n",
    "X_train_scaled,\n",
    "y_train,\n",
    "w,\n",
    "b,\n",
    "method='adam',  # 'vanilla' or 'adam'\n",
    "previous_cost=compute_cost(X_train, y_train, w, b),\n",
    "threshold=1e-6,\n",
    "max_iters=1000000\n",
    ")\n",
    "\n",
    "y_pred_test = predict(X_test_scaled, wf, bf)  # Use your predict function\n",
    "\n",
    "\n",
    "# Compute classification metrics\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "rec = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\n Validation Set Results for Degree 2\")\n",
    "# Print results\n",
    "print(\"\\n📊 Classification Metrics (Validation Set):\")\n",
    "print(f\"Accuracy       : {acc:.4f}\")\n",
    "print(f\"Precision      : {prec:.4f}\")\n",
    "print(f\"Recall         : {rec:.4f}\")\n",
    "print(f\"F1 Score       : {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f76e9",
   "metadata": {},
   "source": [
    "### **Saving Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_weights_to_npy(wf, bf, std, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7db86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting logs to DataFrame\n",
    "df_log = pd.DataFrame({\n",
    "    'iteration': iteration_log,\n",
    "    'cost': cost_log,\n",
    "    'bias': bias_log,\n",
    "})\n",
    "\n",
    "#Saving log\n",
    "#df_log.to_csv('gd_convergence_log.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
