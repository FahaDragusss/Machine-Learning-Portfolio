# ğŸ“Œ Lasso Regression from Scratch (Deployed!)

This project is a subset/small step 1 of implementing Linear Regression, Ridge, Lasso, and ElasticNet from scratch using vectorized NumPy.

It is trained on a real-world dataset and is deployed using Streamlit on Hugging Face Spaces.

---

## ğŸ§  Key Features
- âœ… Fully vectorized implementations (no scikit-learn for training)
- âœ… End-to-end workflow: from EDA â†’ preprocessing â†’ training â†’ deployment
- âœ… Animated visualizations to show model behavior
- âœ… Deployed interactive app via Streamlit + Hugging Face

---

## ğŸ’¡ Why Build These From Scratch?

- **Conceptual Depth**: Learn how regression, gradients, and regularization actually work
- **Transparency**: Every equation is visible in code â€” no black boxes
- **Debugging Practice**: GIFs and cost plots help catch subtle bugs
- **Modularization**: Each part of the ML pipeline is cleanly separated

---

## âš™ï¸ Techniques Used

- âœ… Vectorized Matrix Operations (NumPy)
- âœ… Batch Gradient Descent
- âœ… Feature Scaling and Outlier Removal
- âœ… Regularization: L1 (Lasso), L2 (Ridge), and ElasticNet
- âœ… Soft Thresholding (for L1 penalty)
- âœ… Visualization of Cost & Parameter Convergence
- âœ… RÂ², MAE, MSE, Residual Plots, Actual vs. Predicted Plots

---

## ğŸ”— Live Demo Multiple Linear Regression
ğŸ¯ **[ğŸ‘‰ Try the Streamlit App on Hugging Face](https://huggingface.co/spaces/FahaDragusss/MLR-scratch-streamlit)**

## ğŸ”— Live Demo Lasso Regression
ğŸ¯ **[ğŸ‘‰ Try the Streamlit App on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Lasso-Regression-scratch-streamlit)**

---

## ğŸ“Š Dataset
- Dataset: [Auto MPG Dataset](https://www.kaggle.com/datasets/yasserh/auto-mpg-dataset)
- Preprocessing:
  - Handled missing values
  - Feature scaling
  - Log transformation (unskewing)
  - Outlier Handling
  - Exploratory Data Analysis (EDA)

---

## ğŸ§ª Project Structure

```
Linear Models/
â”‚
â”‚
â”œâ”€â”€ElasticNet/
â”‚  â”‚
â”‚  â”œâ”€â”€ Dataset
â”‚  â”œâ”€â”€ Implementation/
â”‚  â”‚   â”œâ”€â”€ MLR L1-L2_reg.ipynb
â”‚  â””â”€â”€ README.md
â”‚
â”‚
â”‚
â”œâ”€â”€Lasso
â”‚  â”‚
â”‚  â”œâ”€â”€ Analysis and visualization/       # Code used to generate GIFs and plots
â”‚  â”œâ”€â”€ app/                           # Streamlit interface
â”‚  â”‚   â”œâ”€â”€ streamlit_app.py
â”‚  â”‚   â”œâ”€â”€ model.joblib
â”‚  â”‚   â””â”€â”€ requirements.txt
â”‚  â”œâ”€â”€ Dataset 
â”‚  â”œâ”€â”€ DevSet/                        # Prototype implementation on synthetic data
â”‚  â”œâ”€â”€ EDA-&-Preprocessing/
â”‚  â”‚   â””â”€â”€ EDA & preprocessing.ipynb
â”‚  â”œâ”€â”€ Implementation/
â”‚  â”‚   â”œâ”€â”€ MLR Model.ipynb
â”‚  â”œâ”€â”€ Results/                       # Final visuals for presentation
â”‚  â””â”€â”€ README.md
â”‚
â”‚
â”‚
â”œâ”€â”€Ridge/
â”‚  â”‚
â”‚  â”œâ”€â”€ Dataset
â”‚  â”œâ”€â”€ DevSet/                        # Prototype implementation on synthetic data
â”‚  â”œâ”€â”€ EDA-&-Preprocessing/
â”‚  â”‚   â””â”€â”€ EDA & preprocessing.ipynb
â”‚  â”œâ”€â”€ Implementation/
â”‚  â”‚   â”œâ”€â”€ MLR L1_reg.ipynb
â”‚  â””â”€â”€ README.md
â”‚
â”‚
â”‚
â””â”€â”€Traditional/
   â”‚
   â”œâ”€â”€ Analysis and visualization/       # Code used to generate GIFs and plots
   â”œâ”€â”€ app/                           # Streamlit interface
   â”‚   â”œâ”€â”€ streamlit_app.py
   â”‚   â”œâ”€â”€ model.joblib
   â”‚   â””â”€â”€ requirements.txt
   â”œâ”€â”€ Dataset 
   â”œâ”€â”€ DevSet/                        # Prototype implementation on synthetic data
   â”œâ”€â”€ EDA-&-Preprocessing/
   â”‚   â””â”€â”€ EDA & preprocessing.ipynb
   â”œâ”€â”€ Implementation/
   â”‚   â”œâ”€â”€ MLR L2_reg.ipynb
   â”œâ”€â”€ Results/                       # Final visuals for presentation
   â””â”€â”€ README.md
```

---

## ğŸš€ How to Run Locally

```bash
# Step 1: Clone the repo
git clone https://github.com/FahaDragusss/Machine-Learning-Portfolio.git

# Step 2: Navigate to app folder
cd ./app

# Step 3: Install dependencies
pip install -r requirements.txt

# Step 4: Run the Streamlit app
streamlit run app.py
```

---

## ğŸ“ˆ Training Visualizations for Traditional Multiple linear Regression

These project includes animations and plots to visualize the training and convergence behavior of models (Some of them are showcased here): 

---

### ğŸï¸ Regression Line Fitting Animation
Shows how the model adjusts the regression line over time to minimize cost.

![Regression Animation](./Traditional/Results/regression_animation.gif)

---

### ğŸ“‰ Cost Function Convergence
Visualizes the descent of the cost function over iterations, confirming successful optimization.

- **Left plot**: Iterations â‰¤ 5000  
- **Right plot**: Iterations â‰¥ 6000 up to convergence

![Cost Convergence](./Traditional/Results/cost_convergence.gif)

---

### ğŸ§  Weights (Theta) Convergence
Demonstrates how model weights (`Î¸`) stabilize over training iterations.

- **Left plot**: Iterations â‰¤ 5000  
- **Right plot**: Iterations â‰¥ 6000 up to convergence

![Weights Convergence](./Traditional/Results/weights_convergence.gif)

---

## ğŸ” Visualization to evaluate Lasso Regression

---

### ğŸ“‰ Residuals Plot
This scatter plot displays the residuals (i.e., the differences between actual and predicted values). Ideally, the points should appear randomly dispersed around the horizontal line at y = 0. Such a pattern suggests that the model's errors are randomly distributed â€” a key indicator of a well-fitted linear model without systematic bias.

![Residual Scatter Plot](./Lasso/Results/residuals_mlassor.png)

---

### ğŸ“Š Actual vs Predicted Plot
This plot compares the predicted values against the actual ground-truth values. Ideally, the points should align closely along the diagonal line (y = x), indicating that the model is accurately capturing the relationship in the data. Deviations from this line reflect prediction errors and help identify underfitting or overfitting.

![Actual vs Predicted Plot](./Lasso/Results/Actual_vs_Predicted_mlassor.png)

---

## ğŸ“š Learnings & Takeaways
- Gradient descent behaves differently under each regularization type.
- Lasso uses non-differentiable penalties â€” hence not truly gradient-based due to which softsign was implemented.
- Visualizing training metrics greatly enhances interpretability.

---

## ğŸ“¬ Contact
Built by **[FahaDragusss](https://github.com/FahaDragusss)**  
Feel free to reach out for collaboration or feedback!

---

## ğŸ“„ License
This project is licensed under the MIT License.
