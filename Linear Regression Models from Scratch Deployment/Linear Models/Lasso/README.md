# ğŸ“Œ Lasso Regression from Scratch (Deployed on Hugging Face)

This module implements **Lasso Regression** from scratch using NumPy, including model training, evaluation, and deployment with a simple Streamlit interface.

Lasso introduces **L1 regularization**, encouraging sparsity in model coefficients â€” a valuable trait in high-dimensional datasets. While it doesn't outperform the traditional model here, it offers robustness and has been deployed for demonstration.

---

## ğŸŒ Live Demo

> Try the model directly here:  
ğŸ”— [Lasso Regression on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Lasso-Regression-scratch-streamlit)

---

## ğŸ§  Key Features

- âœ… Written completely from scratch â€” no scikit-learn used for training  
- âœ… Implements **L1 regularization** (Lasso)  
- âœ… Trained on cleaned Auto-MPG dataset  
- âœ… **Deployed** using Streamlit and Hugging Face 
- âœ… Fully modular structure for training, evaluation, and visualization  
- âœ… Compared directly with traditional Multiple Linear Regression  

Note : Even though this model didnâ€™t outperform others, I deployed it to demonstrate how different forms of regularization affect model behavior â€” both mathematically and practically.

---

## ğŸ“ Directory Structure

Lasso/
â”‚
â”œâ”€â”€ Analysis and Visualization/ # Code for all visualizations and GIFs
â”‚
â”œâ”€â”€ app/ # Deployed app (Streamlit interface)
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ model.joblib
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Dataset/
â”‚
â”œâ”€â”€ DevSet/ # Early experiments on synthetic data
â”‚
â”œâ”€â”€ EDA-&-Preprocessing/ # Cleaned and processed dataset
â”‚ â”œâ”€â”€ car-mpg.csv
â”‚ â””â”€â”€ EDA & preprocessing.ipynb
â”‚
â”œâ”€â”€ Implementation/ # Training and model evaluation code
â”‚ â””â”€â”€ Lasso Model.ipynb
â”‚ 
â”‚
â”œâ”€â”€ Results/ # Final plots
â”‚
â””â”€â”€ README.md


---

## ğŸ“Š Model Performance Summary

### âœ… Generalization:
- **Test RÂ²**: `0.8998`  
- **Train RÂ²**: `0.8451`  
- Higher test RÂ² suggests strong generalization and no overfitting.

---

### âœ… Error Metrics:
- **MSE increased** slightly from `5.6752` â†’ `5.6786`  
- This is just a **0.02% increase** in average prediction error (based on RMSE).  
- **MAE and MSE** continue to decline during training on both sets, confirming **stable convergence**.

---

### âœ… RÂ² Score Trends:
- **Train RÂ²**: `0.8451 â†’ 0.8451`  
- **Test RÂ²**: `0.8999 â†’ 0.8998`  
- These stable metrics suggest the optimization had already reached an efficient solution.

---

### ğŸ“ˆ Summary:
While the Lasso model **does not outperform** the traditional Multiple Linear Regression model on this dataset, it was successfully implemented and deployed to demonstrate the behavior of **L1 regularization** in action.

---

## ğŸ“Š Evaluation Plots

### ğŸ“‰ Residuals Plot  
The residuals are randomly scattered around the **y = 0** line, showing no visible patterns â€” a sign of good fit.

![Residual Scatter Plot](./Results/residuals_mlassor.png)

---

### ğŸ“Š Actual vs Predicted Plot  
Most data points lie near the **y = x** line, meaning predictions closely match actual values.

![Actual vs Predicted Plot](./Results/Actual_vs_Predicted_mlassor.png)

---

## ğŸ“ Takeaways

- Lasso offers robustness and encourages **sparse feature selection**, useful in complex domains.
- On this dataset, it **does not significantly improve metrics** but maintains stability and generalization.
- Still valuable as a deployed regularized model and a conceptual tool.

---

## ğŸ“¬ Contact

Built by **[FahaDragusss](https://github.com/FahaDragusss)**  
Reach out for discussions, collaborations, or suggestions.

---

## ğŸ“„ License

This module is licensed under the **MIT License**.
