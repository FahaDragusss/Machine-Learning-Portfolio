# ğŸ“ŒPolynomial ElasticNet Regression from Scratch

This module implements **Polynomial ElasticNet Regression** from scratch using only NumPy â€” combining both **L1 (Lasso)** and **L2 (Ridge)** regularization. The ElasticNet algorithm helps balance coefficient shrinkage and sparsity, offering robustness on datasets with multicollinearity or when neither Lasso nor Ridge alone performs optimally.

While the model performs **competitively**, it does **not outperform**  lasso regression on this dataset and was only deployed to showcase the ability to implement such model from scratch.

Moreover, the **adam optimizer** used was also implemented by me. Only Numpy was used for the implementation.

---

## ğŸŒ Live Demo

> Try the model directly here:  
ğŸ”— [ElasticNet Regression on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Poly-elasticnet-regression-scratch-streamlit)

---

## ğŸ§  Key Features

- âœ… Complete implementation using only **NumPy**
- âœ… Trains ElasticNet using **gradient descent** optimized with **adam optimizer**
- âœ… Combines **L1 and L2** penalties to balance between Lasso and Ridge
- âœ… Evaluated on the CO2 Emmisions dataset
- âœ… Visualized residuals and prediction quality
- âœ… **Deployed** using Streamlit and Hugging Face 

Note : Even though this model didnâ€™t outperform others, I deployed it to demonstrate how different forms of regularization affect model behavior â€” both mathematically and practically.

---

## ğŸ“ Directory Structure

ElasticNet/
â”‚
â”œâ”€â”€ Analysis and Visualization/ # Notebook to create animations
â”‚
â”œâ”€â”€ app/ # Deployed app (Streamlit interface)
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ model.joblib
â”‚ â””â”€â”€ requirements.txt
â”‚ 
â”œâ”€â”€ Dataset/ # Subset of the complete data used
â”‚
â”œâ”€â”€ Implementation/ # Model training and Implementation
â”‚ â””â”€â”€ Polynomial ElasticNet Model.ipynb
â”‚
â”œâ”€â”€ Results/ # Final evaluation plots
â”‚
â””â”€â”€ README.md


---

## ğŸ“Š Model Performance Summary

### âœ… Generalization:
- **Test RÂ²**: `0.9984`  
- **Train RÂ²**: `0.9984`  
- The model generalizes well to unseen data.

---

### âœ… Error Metrics:
- **MSE** is also practically the same (5.2493) for Test and (5.2696) for the Train. 
- Both **MSE and MAE** dropped on train/test sets during training, indicating **smooth convergence**.

---

## ğŸ“ˆ Evaluation and Visualization Plots

### ğŸ“‰ Cost convergence
A Cost convergence plot shows how the cost decreases as the model learns after each iteration.

![Residuals Plot](./Results/cost_convergence.gif)

---

### ğŸ“Š Actual vs Predicted  
Points lie close to the **diagonal (y = x)** line, showing strong predictive power and alignment between model output and real data.

![Actual vs Predicted](./Results/Predicted_vs_actual_per.png)

---


### ğŸ“ˆ Summary:
The ElasticNet implementation performs reliably well.

---

## ğŸ“ Takeaways

- ElasticNet serves as a **hybrid regularization model**, combining benefits of Lasso and Ridge.
- Ideal for datasets with **many features** or **high multicollinearity**.

---

## ğŸ“¬ Contact

Built by **[FahaDragusss](https://github.com/FahaDragusss)**  
Reach out for discussions, collaborations, or feedback.

---

## ğŸ“„ License

This module is licensed under the **MIT License**.
