# ğŸ“Œ Polynomial Regression from Scratch (Deployed on Hugging Face)

This module implements **Polynomial Regression** from scratch using NumPy, including model training, evaluation, and deployment with a simple Streamlit interface.

Features were converted to polynomial via a funtion implemented under the name **'polynomial_features'** due to this the run time significantly increased as after converting features to a **degree 5** polynomial the features which were already around 12 grew to more than a 1000 due to interaction terms. Due to which the need for optimization arised and which propelled me to learn Optimization. After learning **RMSprop**, **BGD with Momentum** and **Adam**. I decided to implement **adam optimizer** from scratch.

---

## ğŸŒ Live Demo

> Try the model directly here:  
ğŸ”— [Lasso Regression on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Poly-regression-scratch-streamlit)

---

## ğŸ§  Key Features

- âœ… Written completely from scratch â€” no scikit-learn used for training  
- âœ… Trained using **Batch Gradient descent** and **Adam optimizer** both implemented from scratch
- âœ… Trained on cleaned subset of Vehicle CO2 Emmision dataset 
- âœ… **Deployed** using Streamlit and Hugging Face 
- âœ… Fully modular structure for training, evaluation, and visualization  
- âœ… Compared directly with traditional Polynomial Ridge Regression  

Note : Even though this model didnâ€™t outperform others, I deployed it to demonstrate how different forms of regularization affect model behavior â€” both mathematically and practically.

---

## ğŸ“ Directory Structure

Tradition/
â”‚
â”œâ”€â”€ Analysis and Visualization/ # Code for all visualizations and GIFs
â”‚
â”œâ”€â”€ app/ # Deployed app (Streamlit interface)
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ model.joblib
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Dataset/ # Subset of the main dataset.
â”‚
â”œâ”€â”€ Implementation/ # Training and model evaluation code
â”‚ â””â”€â”€ Polynomial Regression.ipynb
â”‚
â”œâ”€â”€ Results/ # Final plots and .nyp files for model parameters.
â”‚
â””â”€â”€ README.md


---

## ğŸ“Š Model Performance Summary

### âœ… Generalization:
- **Test RÂ²**: `0.9677`  
- **Train RÂ²**: `0.9894`  
- Similar RÂ² suggests strong generalization and no overfitting.

---

### âœ… Error Metrics:
- MSE Test (87.8934) vs MSE Train (21.6996) â€” MSE on test is high perhaps due to the Outlier seen in residual scatter plot. MSE is prone to Outliers. On the other hand MAE:
- MAE Test (5.1873) vs MAE Train (4.0101) â€” MAE on Test is marginally higher.
- **MAE and MSE** continue to decline during training on both sets, confirming **stable convergence**.

---

### ğŸ“ˆ Summary:
The **Polynomial Regression** implemented with the implementation from scratch of the **Adam optimizer** now deployed!

---

## ğŸ“Š Evaluation Plots

### ğŸ“‰  Cost Convergence animation  
We can see how after each iteration the model cost decreases. and converges.

![Cost Convergence](./Results/cost_convergence.gif)

---

### ğŸ“Š Actual vs Predicted Plot  
Most data points lie near the **y = x** line, meaning predictions closely match actual values.

![Actual vs Predicted Plot](./Results/Predicted_vs_actual.png)

---

## ğŸ“š Learnings & Takeaways
- Funtion to convert features to a **degree 'x'** function implemented.
- **Adam optimizer**, **RMSprop**, **BGD with momentum** Learnt.
- **Adam optimizer** implemented from scratch.  
- Model generalization â‰  model superiority â€” metrics must guide deployment choices.  
- Model deployed.

---

## ğŸ“¬ Contact

Built by **[FahaDragusss](https://github.com/FahaDragusss)**  
Reach out for discussions, collaborations, or suggestions.

---

## ğŸ“„ License

This module is licensed under the **MIT License**.
