# ğŸ“Œ Polynomial Regression models from Scratch (Deployed!)

This project is a subset/small step 2 of implementing Linear and Logistic Regression, Ridge, Lasso, and ElasticNet from scratch using vectorized NumPy. In step 2 we move towards Implementing Linear Regression with non linear Features aka Polynomial Regression, Ridge, Lasso and ElasticNet from scratch using vectorized Numpy.

It is trained on a real-world dataset and is deployed using Streamlit on Hugging Face Spaces.

---

## ğŸ§  Key Features
- âœ… Fully vectorized implementations (no scikit-learn for training)
- âœ… End-to-end workflow: from EDA â†’ preprocessing â†’ training â†’ deployment
- âœ… Animated visualizations to show model behavior
- âœ… Deployed interactive app via Streamlit + Hugging Face

---

## ğŸ’¡ Why Build These From Scratch?

- **Conceptual Depth**: Learn how regression, gradients, optimizers/optimization, and regularization actually work
- **Transparency**: Every equation is visible in code â€” no black boxes
- **Debugging Practice**: GIFs and cost plots help catch subtle bugs
- **Modularization**: Each part of the ML pipeline is cleanly separated

---

## âš™ï¸ Techniques Used

- âœ… Vectorized Matrix Operations (NumPy)
- âœ… Batch Gradient Descent
- âœ… Optimizers (Adam optimizer, RMSprop and BGD with Momentum)
- âœ… Feature Scaling and Outlier Removal
- âœ… Converting Features to Polynomial features
- âœ… Created a funtion to convert features to Polynomials from scratch
- âœ… Regularization: L1 (Lasso), L2 (Ridge), and ElasticNet
- âœ… Soft Thresholding (for L1 penalty)
- âœ… Visualization of Cost & Parameter Convergence
- âœ… RÂ², MAE, MSE, Residual Plots, Actual vs. Predicted Plots

---

## ğŸ”— Live Demo Polynomial Regression
ğŸ¯ **[ğŸ‘‰ Try the Streamlit App on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Poly-regression-scratch-streamlit)**

## ğŸ”— Live Demo Polynomial Ridge Regression
ğŸ¯ **[ğŸ‘‰ Try the Streamlit App on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Poly-lasso-regression-scratch-streamlit)**

## ğŸ”— Live Demo Polynomial Lasso Regression
ğŸ¯ **[ğŸ‘‰ Try the Streamlit App on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Poly-ridge-regression-scratch-streamlit)**

## ğŸ”— Live Demo Polynomial ElasticNet Regression
ğŸ¯ **[ğŸ‘‰ Try the Streamlit App on Hugging Face](https://huggingface.co/spaces/FahaDragusss/Poly-elasticnet-regression-scratch-streamlit)**

---

## ğŸ“Š Dataset
- Dataset: [Vehicle CO2 Emmision Dataset](https://www.kaggle.com/datasets/brsahan/vehicle-co2-emissions-dataset)
- Preprocessing:
  - Handled missing values
  - Feature scaling
  - Log transformation (unskewing)
  - Outlier Handling
  - Exploratory Data Analysis (EDA)

---

## ğŸ§ª Project Structure

```
Polynomial Models/
â”‚
â”‚
â”œâ”€â”€ElasticNet/ # Similar to traditional
â”‚
â”œâ”€â”€Lasso/ # Similar to traditional
â”‚
â”œâ”€â”€Ridge/ # Similar to traditional
â”‚
â”œâ”€â”€Preprocessing/
â”‚  â”‚ 
â”‚  â”œâ”€â”€co2.csv # Main dataset
â”‚  â”‚ 
â”‚  â”œâ”€â”€Fuel-type-E.csv # Subset of main dataset
â”‚  â”œâ”€â”€Fuel-type-D.csv # Subset of main dataset
â”‚  â”œâ”€â”€Fuel-type-X.csv # Subset of main dataset
â”‚  â”œâ”€â”€Fuel-type-Z.csv # Subset of main dataset
â”‚  â”œâ”€â”€PER X.ipynb # notebook modeling and test scikit learn models.
â”‚  â”œâ”€â”€PRR D.ipynb # notebook modeling and test scikit learn models.
â”‚  â”œâ”€â”€PRR E.ipynb # notebook modeling and test scikit learn models.
â”‚  â””â”€â”€PRR Z.ipynb # notebook modeling and test scikit learn models.
â”‚ 
â””â”€â”€Traditional/
   â”‚
   â”œâ”€â”€ Analysis and visualization/       # Code used to generate GIFs and plots
   â”œâ”€â”€ app/                           # Streamlit interface
   â”‚   â”œâ”€â”€ streamlit_app.py
   â”‚   â”œâ”€â”€ model.joblib
   â”‚   â””â”€â”€ requirements.txt
   â”œâ”€â”€ Dataset 
   â”œâ”€â”€ DevSet/                        # Prototype implementation on synthetic data
   â”œâ”€â”€ Implementation/
   â”‚   â”œâ”€â”€ Polynomial Regression.ipynb
   â”œâ”€â”€ Results/                       # Final visuals for presentation
   â””â”€â”€ README.md
```

---

## ğŸš€ How to Run Locally

```bash
# Step 1: Clone the repo
git clone https://github.com/FahaDragusss/Machine-Learning-Portfolio.git

# Step 2: Navigate to app folder
cd ./app

# Step 3: Install dependencies
pip install -r requirements.txt

# Step 4: Run the Streamlit app
streamlit run app.py
```

---

## ğŸ“ˆ Training Visualizations for Traditional Multiple linear Regression

These project includes animations and plots to visualize the training and convergence behavior of models (Some of them are showcased here): 

---

### ğŸ“‰ Cost Function Convergence
Visualizes the descent of the cost function over iterations, confirming successful optimization.

![Cost Convergence](./Ridge/Results/cost_convergence.gif)

---

## ğŸ” Visualization to evaluate Lasso Regression

---

### ğŸ“Š Actual vs Predicted Plot
This plot compares the predicted values against the actual ground-truth values. Ideally, the points should align closely along the diagonal line (y = x), indicating that the model is accurately capturing the relationship in the data. Deviations from this line reflect prediction errors and help identify underfitting or overfitting.

![Actual vs Predicted Plot](./ElasticNet/Results/Predicted_vs_actual_per.png)

---

## ğŸ“š Learnings & Takeaways
- Gradient descent behaves differently under each regularization type.
- Gradient descent with Adam optimizer saves so much time. In my case with adam my model converged in one of the examples in 8000 iterations and without it took more that 100000 iterations.
- Lasso uses non-differentiable penalties â€” hence not truly gradient-based due to which softsign was implemented.
- Visualizing training metrics greatly enhances interpretability.

---

## ğŸ“¬ Contact
Built by **[FahaDragusss](https://github.com/FahaDragusss)**  
Feel free to reach out for collaboration or feedback!

---

## ğŸ“„ License
This project is licensed under the MIT License.
